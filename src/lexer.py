#!/usr/bin/env python3
"""
–õ–ï–ö–°–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –ü–°–ï–í–î–û–ö–û–î–ê
–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ1

–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞, —Ä–µ–∞–ª–∏–∑—É—é—â–∏–π —Ä–∞–∑–±–æ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞
–Ω–∞ –ø—Å–µ–≤–¥–æ–∫–æ–¥–µ –Ω–∞ —Ç–æ–∫–µ–Ω—ã —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∏—Ö —Ç–∏–ø–∞ –∏ –ø–æ–∑–∏—Ü–∏–∏.
"""

import re
import os
import sys
from typing import List, Dict, Any

class PseudocodeLexer:
    """
    –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —É—á–µ–±–Ω–æ–≥–æ —è–∑—ã–∫–∞ –ø—Å–µ–≤–¥–æ–∫–æ–¥–∞.
    
    –û—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç —Ä–∞–∑–±–∏–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º:
    - —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∞ (–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ, –æ–ø–µ—Ä–∞—Ç–æ—Ä, –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ —Ç.–¥.)
    - –∑–Ω–∞—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞
    - –ø–æ–∑–∏—Ü–∏–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ (—Å—Ç—Ä–æ–∫–∞, —Å—Ç–æ–ª–±–µ—Ü)
    """
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (–æ—Ç –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∫ –æ–±—â–∏–º)
    TOKEN_SPECIFICATION = [
        # –ú–Ω–æ–≥–æ—Å–∏–º–≤–æ–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–µ—Ä–≤—ã–º–∏!)
        ('LEQ',       r'<='),      # –ú–µ–Ω—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ
        ('GEQ',       r'>='),      # –ë–æ–ª—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ  
        ('EQ',        r'=='),      # –†–∞–≤–Ω–æ
        ('NEQ',       r'!='),      # –ù–µ —Ä–∞–≤–Ω–æ
        ('AND',       r'&&'),      # –õ–æ–≥–∏—á–µ—Å–∫–æ–µ –ò
        ('OR',        r'\|\|'),    # –õ–æ–≥–∏—á–µ—Å–∫–æ–µ –ò–õ–ò
        
        # –û–¥–Ω–æ—Å–∏–º–≤–æ–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        ('LT',        r'<'),       # –ú–µ–Ω—å—à–µ
        ('GT',        r'>'),       # –ë–æ–ª—å—à–µ
        ('ASSIGN',    r'='),       # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–µ
        ('NOT',       r'!'),       # –õ–æ–≥–∏—á–µ—Å–∫–æ–µ –ù–ï
        ('PLUS',      r'\+'),      # –°–ª–æ–∂–µ–Ω–∏–µ
        ('MINUS',     r'-'),       # –í—ã—á–∏—Ç–∞–Ω–∏–µ
        ('MUL',       r'\*'),      # –£–º–Ω–æ–∂–µ–Ω–∏–µ
        ('DIV',       r'/'),       # –î–µ–ª–µ–Ω–∏–µ
        ('MOD',       r'%'),       # –û—Å—Ç–∞—Ç–æ–∫ –æ—Ç –¥–µ–ª–µ–Ω–∏—è
        ('COMMA',     r','),       # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
        ('SEMI',      r';'),       # –ö–æ–Ω–µ—Ü –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        ('LBRACE',    r'\{'),      # –ù–∞—á–∞–ª–æ –±–ª–æ–∫–∞
        ('RBRACE',    r'\}'),      # –ö–æ–Ω–µ—Ü –±–ª–æ–∫–∞
        ('LPAREN',    r'\('),      # –û—Ç–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞
        ('RPAREN',    r'\)'),      # –ó–∞–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        ('IF',        r'\bif\b'),      # –£—Å–ª–æ–≤–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä
        ('ELSE',      r'\belse\b'),    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ç–∫–∞
        ('WHILE',     r'\bwhile\b'),   # –¶–∏–∫–ª while
        ('FOR',       r'\bfor\b'),     # –¶–∏–∫–ª for
        ('IN',        r'\bin\b'),      # –û–ø–µ—Ä–∞—Ç–æ—Ä –≤—Ö–æ–∂–¥–µ–Ω–∏—è
        ('RANGE',     r'\brange\b'),   # –§—É–Ω–∫—Ü–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        ('PRINT',     r'\bprint\b'),   # –í—ã–≤–æ–¥ –Ω–∞ —ç–∫—Ä–∞–Ω
        
        # –õ–∏—Ç–µ—Ä–∞–ª—ã –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        ('STRING',    r'"[^"]*"'),     # –°—Ç—Ä–æ–∫–æ–≤—ã–µ –ª–∏—Ç–µ—Ä–∞–ª—ã
        ('NUMBER',    r'\d+'),         # –¶–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –ª–∏—Ç–µ—Ä–∞–ª—ã
        ('ID',        r'[a-zA-Z_][a-zA-Z_0-9]*'),  # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        
        # –°–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        ('COMMENT',   r'#.*'),         # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        ('NEWLINE',   r'\n'),          # –ü–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏
        ('SKIP',      r'[ \t]+'),      # –ü—Ä–æ–±–µ–ª—ã –∏ —Ç–∞–±—É–ª—è—Ü–∏–∏
        ('MISMATCH',  r'.'),           # –õ—é–±–æ–π –¥—Ä—É–≥–æ–π —Å–∏–º–≤–æ–ª (–æ—à–∏–±–∫–∞)
    ]
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞."""
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤
        token_regex = '|'.join(f'(?P<{name}>{pattern})' for name, pattern in self.TOKEN_SPECIFICATION)
        self.pattern = re.compile(token_regex)
    
    def tokenize(self, code: str) -> List[Dict[str, Any]]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –Ω–∞ —Ç–æ–∫–µ–Ω—ã.
        
        Args:
            code (str): –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –Ω–∞ –ø—Å–µ–≤–¥–æ–∫–æ–¥–µ
            
        Returns:
            List[Dict[str, Any]]: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç:
                - type: —Ç–∏–ø —Ç–æ–∫–µ–Ω–∞
                - value: –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞
                - line: –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏
                - column: –ø–æ–∑–∏—Ü–∏—è –≤ —Å—Ç—Ä–æ–∫–µ
                
        Raises:
            RuntimeError: –ü—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        """
        tokens = []
        line_num = 1
        line_start = 0
        
        for match in self.pattern.finditer(code):
            kind = match.lastgroup
            value = match.group()
            column = match.start() - line_start
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
            if kind == 'SKIP':
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
            elif kind == 'NEWLINE':
                line_num += 1
                line_start = match.end()
                continue
            elif kind == 'COMMENT':
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            elif kind == 'NUMBER':
                value = int(value)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–∞ –≤ int
            elif kind == 'STRING':
                value = value[1:-1]  # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ —É —Å—Ç—Ä–æ–∫
            elif kind == 'MISMATCH':
                raise RuntimeError(f'–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª {value!r} –Ω–∞ —Å—Ç—Ä–æ–∫–µ {line_num}')
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            tokens.append({
                'type': kind,
                'value': value,
                'line': line_num,
                'column': column
            })
        
        return tokens


class LexerAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —É–¥–æ–±–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –ª–µ–∫—Å–µ—Ä–æ–º.
    
    –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞, –≤—ã–≤–æ–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    –∏ —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞."""
        self.lexer = PseudocodeLexer()
        self.tokens = []
    
    def analyze(self, code: str) -> List[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–¥ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤.
        
        Args:
            code (str): –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            List[Dict[str, Any]]: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        self.tokens = []
        raw_tokens = self.lexer.tokenize(code)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–∫–µ–Ω—ã –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        for token in raw_tokens:
            token_info = {
                'text': str(token['value']),
                'type': token['type'],
                'line': token['line'],
                'column': token['column']
            }
            self.tokens.append(token_info)
        
        return self.tokens
    
    def analyze_file(self, file_path: str) -> List[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–¥ –∏–∑ —Ñ–∞–π–ª–∞.
        
        Args:
            file_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∫–æ–¥–æ–º
            
        Returns:
            List[Dict[str, Any]]: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                code = f.read()
            return self.analyze(code)
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return []
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return []
    
    def print_tokens(self, title: str = "–õ–ï–ö–°–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó"):
        """
        –í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Å–∏–≤–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å —Ç–æ–∫–µ–Ω–∞–º–∏.
        
        Args:
            title (str): –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞
        """
        if not self.tokens:
            print("üì≠ –ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return
            
        print("=" * 80)
        print(title)
        print("=" * 80)
        print(f"{'–¢–û–ö–ï–ù':<20} {'–¢–ò–ü':<15} {'–°–¢–†–û–ö–ê':<8} {'–ü–û–ó–ò–¶–ò–Ø':<8}")
        print("-" * 80)
        
        for token in self.tokens:
            print(f"{token['text']:<20} {token['type']:<15} {token['line']:<8} {token['column']:<8}")
        
        print("-" * 80)
        print(f"üìä –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(self.tokens)}")
    
    def get_statistics(self) -> Dict[str, int]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º —Ç–æ–∫–µ–Ω–æ–≤.
        
        Returns:
            Dict[str, int]: –°–ª–æ–≤–∞—Ä—å {—Ç–∏–ø_—Ç–æ–∫–µ–Ω–∞: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ}
        """
        token_types = {}
        for token in self.tokens:
            token_type = token['type']
            token_types[token_type] = token_types.get(token_type, 0) + 1
        return token_types
    
    def print_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º —Ç–æ–∫–µ–Ω–æ–≤."""
        stats = self.get_statistics()
        
        if not stats:
            print("üì≠ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
            return
            
        print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ò–ü–ê–ú –¢–û–ö–ï–ù–û–í:")
        print("-" * 40)
        for token_type, count in sorted(stats.items()):
            print(f"  {token_type}: {count}")
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_tokens = len(self.tokens)
        unique_types = len(stats)
        most_common = max(stats, key=stats.get) if stats else "N/A"
        
        print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"  –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤: {unique_types}")
        print(f"  –°–∞–º—ã–π —á–∞—Å—Ç—ã–π —Ç–∏–ø: {most_common} ({stats.get(most_common, 0)} —Ä–∞–∑)")


def demonstrate_capabilities():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞.
    """
    print("üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –í–û–ó–ú–û–ñ–ù–û–°–¢–ï–ô –õ–ï–ö–°–ï–†–ê")
    print("=" * 60)
    
    test_cases = [
        ("–ü—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–µ", "x = 42;"),
        ("–ê—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞", "result = (a + b) * c / 2;"),
        ("–û—Å—Ç–∞—Ç–æ–∫ –æ—Ç –¥–µ–ª–µ–Ω–∏—è", "remainder = a % b;"),
        ("–°—Ä–∞–≤–Ω–µ–Ω–∏—è", "if (x >= 10 && y != 5) { z = 1; }"),
        ("–¶–∏–∫–ª for", "for i in range(1, 10) { print(i); }"),
        ("–¶–∏–∫–ª while", "while (x > 0 || y < 100) { x = x - 1; }"),
        ("–°—Ç—Ä–æ–∫–∏", 'print("Hello, World!");'),
        ("–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏", "# –≠—Ç–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π\nx = 10; # –ò —ç—Ç–æ —Ç–æ–∂–µ"),
    ]
    
    analyzer = LexerAnalyzer()
    
    for name, code in test_cases:
        print(f"\nüîπ {name}:")
        print(f"   –ö–æ–¥: {code}")
        tokens = analyzer.analyze(code)
        token_descriptions = [f"{t['type']}({t['text']})" for t in tokens]
        print(f"   –¢–æ–∫–µ–Ω—ã: {token_descriptions}")
        print(f"   –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")


def analyze_example_code():
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∫–æ–¥–∞ –Ω–∞ –ø—Å–µ–≤–¥–æ–∫–æ–¥–µ.
    """
    print("\n" + "=" * 60)
    print("üìä –ê–ù–ê–õ–ò–ó –ü–†–ò–ú–ï–†–ê –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –ö–û–î–ê")
    print("=" * 60)
    
    example_code = """# –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –Ω–∞ –ø—Å–µ–≤–¥–æ–∫–æ–¥–µ
# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—É–º–º—ã —á–µ—Ç–Ω—ã—Ö —á–∏—Å–µ–ª

sum = 0;
n = 10;

for i in range(1, n + 1) {
    if (i % 2 == 0) {
        sum = sum + i;
        print("Added: " + i);
    }
}

print("Total sum: " + sum);

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
if (sum > 20) {
    print("Large sum!");
} else {
    print("Small sum.");
}
"""
    
    analyzer = LexerAnalyzer()
    tokens = analyzer.analyze(example_code)
    analyzer.print_tokens("–ê–ù–ê–õ–ò–ó –ü–†–ò–ú–ï–†–ù–û–ô –ü–†–û–ì–†–ê–ú–ú–´")
    analyzer.print_statistics()


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞.
    """
    print("üéØ –õ–ï–ö–°–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –ü–°–ï–í–î–û–ö–û–î–ê - –õ–†1")
    print("=" * 60)
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
    demonstrate_capabilities()
    
    # –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
    analyze_example_code()
    
    print("\n" + "=" * 60)
    print("‚úÖ –õ–ï–ö–°–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –†–ê–ë–û–¢–ê–ï–¢ –ö–û–†–†–ï–ö–¢–ù–û!")
    print("=" * 60)


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nüí• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
