#!/usr/bin/env python3
import sys
import os
from antlr4 import *

try:
    from FixedPseudocodeLexer import FixedPseudocodeLexer
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    print("–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: python fixed_setup.py –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç 2")
    sys.exit(1)

class FixedLexerAnalyzer:
    def __init__(self):
        self.tokens_info = []
    
    def analyze(self, code):
        self.tokens_info = []
        input_stream = InputStream(code)
        lexer = FixedPseudocodeLexer(input_stream)
        tokens = lexer.getAllTokens()
        
        for token in tokens:
            token_type = lexer.symbolicNames[token.type]
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            if token_type == 'WS':
                continue
                
            token_info = {
                'text': token.text,
                'type': token_type,
                'line': token.line,
                'column': token.column
            }
            self.tokens_info.append(token_info)
        
        return self.tokens_info
    
    def print_tokens(self):
        if not self.tokens_info:
            print("–ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return
            
        print("=" * 80)
        print("–õ–ï–ö–°–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó (Fixed ANTLR Lexer)")
        print("=" * 80)
        print(f"{'–¢–û–ö–ï–ù':<20} {'–¢–ò–ü':<20} {'–°–¢–†–û–ö–ê':<8} {'–ü–û–ó–ò–¶–ò–Ø':<8}")
        print("-" * 80)
        
        for token in self.tokens_info:
            print(f"{token['text']:<20} {token['type']:<20} {token['line']:<8} {token['column']:<8}")
        
        print("-" * 80)
        print(f"–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(self.tokens_info)}")

def test_fixed_lexer():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π ANTLR –ª–µ–∫—Å–µ—Ä"""
    print("üß™ –¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ ANTLR –ª–µ–∫—Å–µ—Ä–∞")
    print("-" * 40)
    
    test_cases = [
        "x = 10;",
        "if (x >= 5) { y = 1; }",
        'print("Hello");'
    ]
    
    analyzer = FixedLexerAnalyzer()
    
    for i, code in enumerate(test_cases, 1):
        print(f"\n–¢–µ—Å—Ç {i}: {code}")
        try:
            tokens = analyzer.analyze(code)
            
            for token in tokens:
                print(f"  '{token['text']}' -> {token['type']}")
            
            print(f"  –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")

def main():
    print("üéØ –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Å–µ–≤–¥–æ–∫–æ–¥–∞ (Fixed ANTLR Version)")
    print("=" * 50)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª
    test_fixed_lexer()
    
    print("\n" + "=" * 50)
    print("üìä –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–º–µ—Ä–∞ –∫–æ–¥–∞")
    print("=" * 50)
    
    # –ü—Ä–∏–º–µ—Ä –ø—Å–µ–≤–¥–æ–∫–æ–¥–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    test_code = """
x = 10;
if (x >= 5) {
    y = x * 2;
    print("Result");
}
"""
    
    analyzer = FixedLexerAnalyzer()
    try:
        tokens = analyzer.analyze(test_code)
        analyzer.print_tokens()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Ç–æ–∫–µ–Ω–æ–≤
        token_types = {}
        for token in tokens:
            token_type = token['type']
            token_types[token_type] = token_types.get(token_type, 0) + 1
        
        print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ò–ü–ê–ú –¢–û–ö–ï–ù–û–í:")
        for token_type, count in sorted(token_types.items()):
            print(f"  {token_type}: {count}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")

if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
